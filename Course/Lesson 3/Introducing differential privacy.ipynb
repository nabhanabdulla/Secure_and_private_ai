{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Data Privacy`\n",
    "2. `Differential Privacy`\n",
    "3. \n",
    "\n",
    "<p style=\"font-size:20px;font-family:verdana;line-height:20pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Why data privacy?**\n",
    "\n",
    "    * Who doesn't want their data to be private :)\n",
    "    \n",
    "    \n",
    "* **Challenges for data privacy**:\n",
    "\n",
    "    * On one hand we are searching for more and more high-quality models and accessible data while OTOH we need to keep data safe from both intentional and accidental leakage\n",
    "     \n",
    "     \n",
    "* **Why most datasets are isolated from the majority and accessed within large enterprises?**\n",
    "\n",
    "    1. Enterprises have a legal risk which prevents them from wanting to share their data set outside of their organization.\n",
    "    2. Enterprises have a competitive advantage to hang on to large datasets collected from or about their customers.\n",
    "\n",
    "    *This leads to a situation where most researchers are extremely constrained in terms of the amount of data they have access to to solve their problems*\n",
    "\n",
    "\n",
    "* **Implications?**\n",
    "\n",
    "    * Quietly holding back research across society making it more challenging to cure disease or understand complex societal trends\n",
    "    * Data that are most personal are more restricted and hence the most important and personal issues in society simply cannot be addressed with machine learning\n",
    "\n",
    "\n",
    "* **Workaround!**:\n",
    "\n",
    "    *Learning how to do machine learning that protects privacy*\n",
    "    \n",
    "**NB**: `Federated learning` and `Differential privacy` are some of the ideas involved ( incase you are wondering )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The general goal of differential privacy is to ensure that different types of **statistical analysis** doesn't compromise privacy\n",
    "\n",
    "* This means that our statistical analysis of some training data or database doesn't compromise the **privacy** of any particular individual contained within the dataset\n",
    "\n",
    "* **At a higher level, we aren't trying to protect information but to protect people**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to maintain privacy we first need to propose a robust definition for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p style=\"font-size:14px;font-family:verdana;line-height:20pt\">**Contender 1**: \"Privacy is preserved if, after the analysis, the analyzer doesn't know *anything* about the people in the dataset\"</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis*:  \n",
    "\n",
    "* If you think about this definition of privacy in the context of your home and why you have blinds in the front door, it's to ensure that information about you doesn't leak **at all**\n",
    "\n",
    "\n",
    "* But in the context of statistics this is really insufficient as in all cases of ML we are trying to **learn something new** about a dataset w/o learning specific things about individuals in it which might be sensitive or harm them in any way\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p style=\"font-size:14px;font-family:verdana;line-height:20pt\">**Contender 2**: \"Anything that can be learned from a participant in a statistical database, can be learned without access to the database\" </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis*:\n",
    "    \n",
    "* By this definition we would have preserved individuals privacy\n",
    "\n",
    "\n",
    "* In the context of statistics analysis, this definition means that anything you actually learn about a person should only be **public information**\n",
    "\n",
    "\n",
    "* Imagine if we're trying to train a classifier to predict cancer in CT scans but these CT scans haven't been published anywhere else. So we train our deep neural network to learn how to classify cancer in a CT scan. Now, if we ask the question if this classifier violate privacy, Yes! We learned all the info from a private dataset that doesn't exist anywhere else. Thus according to this definition,  information from this private dataset was leaked even though we only leaked information pertaining to the general classification of what tumors look like\n",
    "\n",
    "\n",
    "* Finally, this definition assumes that information which was been made public elsewhere isn't harmful to an individual. Consider if it's been made slightly public, maybe in some circles that an individual has a disease. Clearly this does not mean that it's now our right to continue to propagate this information.\n",
    "\n",
    "\n",
    "* So this definition fails for two reasons:\n",
    "\n",
    "    1. First, it prevents us from learning **anything** from private datasets( why use them then? )\n",
    "\n",
    "    2. It allows us to learn and propagate information just because it's already public which is in the best case impossible to know and in the worst case, harmful information to spread even further.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p style=\"font-size:20px;font-family:verdana;line-height:20pt\">**Winner**: \"Differential privacy\" describes a promise, made by a data holder, or curator, to a data subject, and the promise is like this, \n",
    "            <span style=\"font-size:20px;font-style:italic\">\"You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.\"<span> - Cynthia Dwork</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say we are analysing a database with records of let's say if individuals have or donot have cancer( or some other sensitive property for that matter ). In the context of this DB, we can define privacy as given that we are performing a query against the DB\n",
    "> <p style=\"font-size:20px;font-family:verdana;line-height:15pt\">If we remove a person from the DB, and query doesn't change, then that person's privacy is fully protected</p>\n",
    "* i.e, if the query doesn't change even after we remove a person from the DB, then the person wasn't leaking any statistical information into the output of the query\n",
    "> <p style=\"font-size:18px;font-family:verdana;line-height:15pt\">Can we construct a query that doesn't change no matter who we remove from the DB?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Key to the definition of differenital privacy is the ability to ask the question \n",
    "> <p style=\"font-size:14px;font-family:verdana;line-height:20pt\">\n",
    "    When querying a database, if I removed someone from the database, would the output of the query be any different?\n",
    "</p>\n",
    "\n",
    "* Thus, in order to check this, we must construct what we term **parallel databases** which are simply databases with one entry removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 Sensitivity**: is the *maximum* amount the query changes when removing and individual from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The functions we have seen earlier like `sum()` and `mean()` had constant sensitivity while this **threshold** function has *variable sensitivity*. \n",
    "* It's 1 when the sum of original db is 6 and 0 when the sum of original db is significantly below or above six. \n",
    "* So, even though the maximum sensitivity **threshold** function can have is 1, it depends on the data too( ie can be zero ) - **data conditioned sensitivity**\n",
    "\n",
    "\n",
    "* Non data conditioned sensitivity: sensitivity just based on the function and the potential range of data\n",
    "* Data conditioned sensitivity: sensitivity that takes into account not only the potential range of data but the actual values in the database itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding random noise to the db and to the queries in the db is one of the main strategy we use to protect individual's privacy\n",
    "\n",
    "* Two different kinds of different privacy( referring to where we add noise ):\n",
    "        1. Local DP: where noise is added to each individual data point. This can be thought of as adding noise directly to the db or even individuals add noise to their own data b4 even sending it to the db - users doesn't necessarily have to trust the db owner\n",
    "        2. Global DP: here noise is added not to each individual data point but to the output of the query on the database. This means that the db itself contains private information and that it's only the interface to the data that adds the noise necessary to protect the data\n",
    "        \n",
    "* If the db owner is trushworthy(ie, should add noise properly), Global DP leads to more accurate results with same level of privacy protection\n",
    "\n",
    "* **Trusted Curator**: An owner of a db on which Global DP is applied. They are trusted to apply DP correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Differential privacy always require some form of randomness or noise added to the query(either directly or by randomness introduced to the data) to protect from things like differencing attacks\n",
    "\n",
    "* **Randomized response**: Technique used in social sciences when trying to learn about high level trends for a taboo behaviour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local and Global DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The method of adding noise to individual data points is \"Local Differential Privacy\". \n",
    "* This is necessary for some situations wherein the data is SO sensitive that individuals do not trust noise to be added later. However, it comes at a very high cost in terms of accuracy.\n",
    "\n",
    "\n",
    "* Alternatively we can add noise AFTER data has been aggregated by a function, this is \"Global Differential Privacy\"\n",
    "* This kind of noise can allow for similar levels of protection with a lower affect on accuracy. \n",
    "* However, participants must be able to trust that no-one looked at their datapoints before the aggregation took place. \n",
    "* In some situations this works out well, in others (such as an individual hand-surveying a group of people), this is less realistic.\n",
    "\n",
    "**Nevertheless, global differential privacy is incredibly important because it allows us to perform differential privacy on smaller groups of individuals with lower amounts of noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea here is that we want to add noise to the output of our function. We actually have two different kinds of noise we can add - Laplacian Noise or Gaussian Noise. However, before we do so at this point we need to dive into the formal definition of Differential Privacy.\n",
    "\n",
    "![alt text](dp_formula.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definition does not _create_ differential privacy, instead it is a measure of how much privacy is afforded by a query M. Specifically, it's a comparison between running the query M on a database (x) and a parallel database (y). As you remember, parallel databases are defined to be the same as a full database (x) with one entry/person removed.\n",
    "\n",
    "Thus, this definition says that FOR ALL parallel databases, the maximum distance between a query on database (x) and the same query on database (y) will be e^epsilon, but that occasionally this constraint won't hold with probability delta. Thus, this theorem is called \"epsilon delta\" differential privacy.\n",
    "\n",
    "#### Epsilon\n",
    "\n",
    "Let's unpack the intuition of this for a moment. \n",
    "\n",
    "Epsilon Zero: If a query satisfied this inequality where epsilon was set to 0, then that would mean that the query for all parallel databases outputed the exact same value as the full database. As you may remember, when we calculated the \"threshold\" function, often the Sensitivity was 0. In that case, the epsilon also happened to be zero.\n",
    "\n",
    "Epsilon One: If a query satisfied this inequality with epsilon 1, then the maximum distance between all queries would be 1 - or more precisely - the maximum distance between the two random distributions M(x) and M(y) is 1 (because all these queries have some amount of randomness in them, just like we observed in the last section).\n",
    "\n",
    "#### Delta\n",
    "\n",
    "Delta is basically the probability that epsilon breaks. Namely, sometimes the epsilon is different for some queries than it is for others. For example, you may remember when we were calculating the sensitivity of threshold, most of the time sensitivity was 0 but sometimes it was 1. Thus, we could calculate this as \"epsilon zero but non-zero delta\" which would say that epsilon is perfect except for some probability of the time when it's arbitrarily higher. Note that this expression doesn't represent the full tradeoff between epsilon and delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we have more _private_ data we can achieve higher accuracy and privacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP for DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect Privacy**\n",
    "> A Query to a database returns the same value even if we remove any person from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfect Privacy(AI Model)**\n",
    "> Training a model on a dataset should return the same model even if we remove any person from the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In essence, the training process is a kind of query. However, one should note that this adds two points of complexity which database queries did not have:\n",
    "\n",
    "    1. Do we always know where **people** are referenced in the dataset?\n",
    "        * In the database example we knew that each person was referenced by a single row and hence could easily create a parallel db to check the query\n",
    "        \n",
    "    2. Neural models rarely never train to the same output model, even on identical data\n",
    "\n",
    "The answer to (1) is to treat each training example as a single, separate person. Strictly speaking, this is often overly zealous as some training examples have no relevance to people and others may have multiple/partial (consider an image with multiple people contained within it). Thus, localizing exactly where \"people\" are referenced, and thus how much your model would change if people were removed, is challenging.\n",
    "\n",
    "The answer to (2) is also an open problem - but several interesitng proposals have been made. We're going to focus on one of the most popular proposals, PATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Privacy Budget**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How much epsilon/delta leakage we allow for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise to be added depends on:\n",
    "* Type of noise added(Laplacian/Gaussian)\n",
    "* Sensitivity of the query\n",
    "* Epsilon\n",
    "* Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Laplacian noise,\n",
    "\n",
    "    b = sensitivity(query)/epsilon\n",
    "    \n",
    "b - amount of noise to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
