{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to PyTorch - Udacity\n",
    "\n",
    "1. `Reshaping Tensors`\n",
    "2. `Numpy to Torch and back`\n",
    "3. `Getting Datasets`\n",
    "4. `2-Layer NN from SCRATCH`\n",
    "5. `Softmax from SCRATCH`\n",
    "    * Broadcasting in Numpy INTUITION\n",
    "    \n",
    "    \n",
    "6. `Creating ready-made NN( inheriting torch.nn.Module class )`\n",
    "7. `Creating ready-made NN( using nn.Sequential )`\n",
    "8. `Losses in PyTorch`\n",
    "9. `Autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reshaping Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let **weights** be a tensor\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data(points to the same memory location) as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8947]) tensor([-0.3227])\n",
      "id(x): 2204467581432, id(y): 2204497406712\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1)\n",
    "y = torch.randn(1)\n",
    "\n",
    "print(x, y)\n",
    "print(f'id(x): {id(x)}, id(y): {id(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.5720]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x = x + y # Normal operation \n",
    "print(f'x = {x} id(x) = {id(x)}') # New location for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.2493]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x += y # inplace operation\n",
    "print(f'x = {x} id(x) = {id(x)}') # existing location used(in-place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([-0.0734]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x.add_(y) # inplace operation\n",
    "print(f'x = {x} id(x) = {id(x)}') # existing location used(in-place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inplace operations in pytorch are always postfixed with a _ , like .add_() or .scatter_(). Python operations like += or *= are also inplace operations.**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numpy to Torch and back\n",
    "\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62610611, 0.73824629, 0.68162487],\n",
       "       [0.45799142, 0.81512616, 0.9046208 ],\n",
       "       [0.13016165, 0.132936  , 0.03448641],\n",
       "       [0.46551398, 0.73371913, 0.05358193]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6261, 0.7382, 0.6816],\n",
       "        [0.4580, 0.8151, 0.9046],\n",
       "        [0.1302, 0.1329, 0.0345],\n",
       "        [0.4655, 0.7337, 0.0536]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62610611, 0.73824629, 0.68162487],\n",
       "       [0.45799142, 0.81512616, 0.9046208 ],\n",
       "       [0.13016165, 0.132936  , 0.03448641],\n",
       "       [0.46551398, 0.73371913, 0.05358193]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2522, 1.4765, 1.3632],\n",
       "        [0.9160, 1.6303, 1.8092],\n",
       "        [0.2603, 0.2659, 0.0690],\n",
       "        [0.9310, 1.4674, 0.1072]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.25221221, 1.47649258, 1.36324974],\n",
       "       [0.91598285, 1.63025232, 1.8092416 ],\n",
       "       [0.26032331, 0.26587199, 0.06897282],\n",
       "       [0.93102797, 1.46743827, 0.10716386]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "# transforms.Normalize((mean_pixel), (standard_deviation_pixel))\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "print(len(trainloader))\n",
    "\n",
    "# Download and load the test data => train=False\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=64)\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = images.view(images.shape[0], -1)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2-Layer NN from SCRATCH\n",
    "\n",
    "Build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer and leave the output without one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([[ 1.2275e+00,  1.7868e+01, -3.3610e+00,  9.6572e+00, -4.4323e+00,\n",
      "          2.3538e+00, -1.7093e+01,  1.1102e+01,  7.5253e+00,  6.0052e+00],\n",
      "        [ 1.9197e+00,  3.9145e+00, -4.1184e+00,  1.3343e+01, -1.7035e+01,\n",
      "         -7.0955e+00, -2.2273e+01,  6.2673e+00, -9.8147e-01,  1.0414e+01],\n",
      "        [ 8.1331e+00,  1.6488e+01, -2.4356e-01,  7.5493e+00, -7.8329e+00,\n",
      "          7.8338e+00, -9.8890e+00, -3.9259e+00,  9.7671e+00, -1.7121e+00],\n",
      "        [-8.2881e+00,  8.3918e+00,  2.4722e+00,  1.5421e+01, -5.5535e+00,\n",
      "         -1.6099e+01, -1.6919e+01,  8.2451e+00,  7.3715e+00,  6.4537e+00],\n",
      "        [-3.0707e+00,  1.1511e+01,  5.7331e+00,  9.1911e+00, -1.1015e+01,\n",
      "         -1.7204e+01, -1.1893e+01,  1.2052e+01,  5.1869e+00,  5.8618e+00],\n",
      "        [ 8.3273e+00,  2.2861e+01,  5.9417e+00,  4.5867e+00, -8.6057e-01,\n",
      "          4.6767e-01, -5.5814e+00,  2.7008e+00,  5.5566e+00, -1.2963e+00],\n",
      "        [ 6.5105e+00,  1.0396e+01,  1.5333e+00,  9.1974e+00, -1.2533e+01,\n",
      "         -5.9950e+00, -9.0925e+00,  5.3603e+00,  9.2596e-01,  8.9151e+00],\n",
      "        [-1.3448e+00,  1.4160e+01, -3.9251e-01,  1.1068e+01, -1.1941e+01,\n",
      "          4.1885e+00, -1.2755e+01,  1.1375e+00,  4.4876e+00, -2.5309e+00],\n",
      "        [-2.0910e-02,  7.1224e+00, -1.8648e+00,  6.9049e+00, -7.3644e+00,\n",
      "         -8.4432e+00, -7.2797e+00, -1.0689e+01,  1.0118e+01,  8.2918e+00],\n",
      "        [-5.9524e+00,  1.5381e+01, -5.1827e+00,  1.1216e+01, -1.1935e+01,\n",
      "          3.8111e+00, -1.1889e+01, -5.8222e+00,  5.1292e+00, -1.1950e+00],\n",
      "        [ 3.0741e+00,  6.2451e+00, -1.4843e+01,  8.5791e+00, -1.2331e+01,\n",
      "         -6.2563e+00, -1.4550e+01,  7.6374e-01, -2.9410e+00,  9.4737e-01],\n",
      "        [-9.6433e-01,  1.1856e+01, -2.1460e+00,  7.4837e+00, -5.3113e+00,\n",
      "          6.4434e+00, -2.4769e+01, -6.4022e+00,  5.6151e+00, -3.4949e-01],\n",
      "        [ 4.5334e+00,  1.2757e+01,  3.2517e+00,  1.4420e+01, -1.7858e+01,\n",
      "         -1.4930e+00, -1.6957e+01, -1.7111e+00,  4.3772e+00,  5.1503e-01],\n",
      "        [ 5.5636e+00,  1.0455e+01, -2.3118e+00,  1.4103e+01, -1.9643e+01,\n",
      "          8.1362e-01, -1.2627e+01,  1.5108e+00,  2.1505e+00, -1.0258e+00],\n",
      "        [ 1.7004e+01,  1.0788e+01,  1.0859e+01,  1.2053e+01, -1.1350e+01,\n",
      "         -1.4089e+01, -1.6695e+01,  1.0260e+01,  5.7475e+00,  7.7414e+00],\n",
      "        [-4.0792e+00,  1.1948e+01, -2.8409e+00,  1.1287e+01, -4.9033e+00,\n",
      "         -9.2723e+00, -4.2669e+00,  8.0237e+00, -6.6854e-02,  2.9975e+00],\n",
      "        [-2.9292e+00,  8.5190e+00,  8.8474e-01,  8.4142e+00, -4.9318e+00,\n",
      "         -1.9509e+00, -1.9224e+01,  1.8102e+00,  6.8603e+00, -9.2254e-01],\n",
      "        [ 5.1781e+00,  1.9556e+01, -3.5521e+00,  1.7280e+01, -1.7311e+00,\n",
      "         -3.6414e+00, -1.5468e+01,  6.6664e+00,  9.0824e+00,  7.2131e+00],\n",
      "        [ 6.1628e+00,  1.4270e+01, -8.5276e+00,  7.3227e+00, -1.0694e+01,\n",
      "         -4.3128e+00, -1.0635e+01,  4.5849e+00,  2.8945e+00,  9.5826e+00],\n",
      "        [ 5.8789e+00, -4.7597e-01, -3.4200e+00,  7.4879e+00, -7.0667e+00,\n",
      "         -5.7030e+00, -1.3984e+01, -1.8884e+00,  9.1656e+00,  5.0976e+00],\n",
      "        [ 3.7618e+00,  2.3547e+01, -3.4180e-01,  1.0696e+01, -6.1304e+00,\n",
      "          4.1629e-01, -1.3966e+01,  2.8627e+00,  5.2352e+00,  3.8309e+00],\n",
      "        [-1.0740e+00,  1.4181e+01, -7.4652e+00,  9.4380e+00, -1.0779e+01,\n",
      "         -4.4312e+00, -1.3663e+01,  2.3071e+00,  4.0605e+00,  1.1501e+01],\n",
      "        [ 8.4703e+00,  1.3298e+01,  1.1895e+00,  9.9966e+00, -4.9283e+00,\n",
      "         -2.1353e+00, -1.5580e+01,  3.9758e+00,  7.8917e+00, -4.9013e+00],\n",
      "        [ 1.3220e+00,  5.9659e+00,  8.8072e+00,  1.1382e+01, -5.4908e+00,\n",
      "         -3.7617e+00, -1.2509e+01,  1.6302e+00,  2.9756e-01,  1.3014e+01],\n",
      "        [ 5.7933e+00,  9.9503e+00, -3.5959e+00,  8.4717e+00, -1.4028e+01,\n",
      "          3.4240e+00, -1.7716e+01, -4.2401e+00,  5.8114e+00,  7.8385e+00],\n",
      "        [-1.6460e+00,  7.6542e+00, -9.1470e+00,  8.7447e+00, -1.1755e+01,\n",
      "          6.6537e-01, -1.5601e+01,  3.3210e+00,  3.3652e+00,  3.1737e+00],\n",
      "        [ 6.5072e+00,  2.4765e+01, -2.1521e+00,  9.4189e+00, -1.2246e+01,\n",
      "         -2.6575e+00, -1.3568e+01, -8.0640e+00,  7.6426e+00,  1.1201e+00],\n",
      "        [-4.5313e+00,  4.8037e+00, -4.9487e+00,  1.7642e+01, -5.6682e+00,\n",
      "         -5.1015e+00, -1.5955e+01, -2.5951e-01,  5.5312e+00,  1.3628e+01],\n",
      "        [-7.0681e-01,  1.4221e+01,  2.0482e+00,  6.7972e+00, -1.4271e+01,\n",
      "         -3.4375e+00, -8.1188e+00,  2.8644e+00,  1.0563e+00,  9.2446e+00],\n",
      "        [-3.3958e-01,  9.9580e+00, -1.2774e+01,  1.4012e+01, -8.3605e+00,\n",
      "         -7.6463e+00, -1.3899e+01, -1.5214e+01,  7.9080e+00, -7.9479e+00],\n",
      "        [ 6.5130e+00,  6.7572e+00, -8.9659e+00,  1.7952e+01, -1.2763e+01,\n",
      "         -3.9656e+00, -1.5732e+01,  4.8910e+00,  6.4145e+00,  1.0895e+01],\n",
      "        [-4.5738e+00,  1.3204e+01, -3.2457e+00,  5.8317e+00, -7.2960e+00,\n",
      "         -5.3711e+00, -1.4109e+01,  1.1324e+01,  1.5403e+00, -1.2591e+00],\n",
      "        [-3.0860e-01,  1.3455e+01, -2.3819e+00,  8.4273e+00, -9.2483e+00,\n",
      "         -2.3050e+00, -2.8579e+01,  9.8449e+00, -1.9906e+00,  1.1598e+01],\n",
      "        [ 1.3743e+01,  1.5186e+01, -2.1097e+00,  1.1746e+01, -1.4385e+01,\n",
      "         -8.6699e+00, -2.1463e+01, -2.1164e+00, -3.3917e+00,  6.7177e+00],\n",
      "        [ 2.4546e+00,  1.4810e+01, -2.2543e+00,  9.7937e+00,  5.4866e+00,\n",
      "         -4.0623e-01, -1.4811e+01, -3.4243e+00, -4.9088e-01,  3.0173e+00],\n",
      "        [-3.9211e+00,  1.7737e+01, -4.5324e+00,  1.1481e+01, -1.1865e+01,\n",
      "         -3.4042e+00, -1.5277e+01,  1.8949e+00,  2.5331e+00,  4.7170e+00],\n",
      "        [-6.3210e+00,  9.1610e+00, -3.7144e+00,  1.6855e+01, -1.0198e+01,\n",
      "          2.9097e+00, -1.9646e+01, -5.2721e+00,  3.7554e+00,  1.1439e+01],\n",
      "        [-1.7213e+00,  1.4224e+01,  6.4338e+00,  1.7415e+01, -8.3898e+00,\n",
      "         -5.8338e+00, -1.2869e+01,  1.0893e+01, -6.9176e-01,  2.2146e+00],\n",
      "        [ 2.3754e+00,  1.1297e+01,  1.4160e+00,  1.0938e+01, -6.0994e+00,\n",
      "          4.8036e+00, -1.4597e+01,  1.7066e+01,  2.6633e+00,  9.7969e+00],\n",
      "        [-1.7198e-01,  1.2201e+01,  6.5993e+00,  3.7973e+00, -1.7752e+01,\n",
      "         -2.4998e+00, -1.3387e+01,  7.7498e+00,  9.6752e+00,  3.4090e+00],\n",
      "        [-1.7270e+00,  1.0049e+01,  4.3021e+00,  1.0012e+01, -1.6718e+01,\n",
      "         -1.6270e+00, -1.2434e+01,  5.0648e-01,  6.5847e+00,  8.7103e+00],\n",
      "        [-3.9118e+00,  6.5003e+00, -1.4821e-01,  1.3538e+01, -9.5462e+00,\n",
      "          5.7484e+00, -1.5109e+01,  1.2891e+00,  5.4181e+00,  5.5239e+00],\n",
      "        [ 3.4923e+00,  7.0976e+00,  7.2264e+00,  1.7651e+01, -1.6620e+01,\n",
      "         -7.9887e+00, -1.5537e+01, -3.7094e-01,  8.2754e-02,  1.0746e+01],\n",
      "        [-3.3089e+00,  1.2866e+01,  3.8452e-01,  5.8648e+00, -1.6589e+01,\n",
      "         -1.8134e+00, -1.4402e+01, -5.2727e+00,  3.7298e-02, -3.6011e+00],\n",
      "        [-3.0780e+00,  1.5458e+00, -7.6187e+00,  9.2200e+00, -7.7033e+00,\n",
      "          6.7173e+00, -1.1830e+01, -1.1704e+00, -8.1303e+00,  1.0303e+01],\n",
      "        [-1.8077e+00,  7.7820e+00, -9.7886e+00,  8.2668e+00, -6.5495e+00,\n",
      "         -7.4815e+00, -1.6830e+01,  2.5089e+00,  1.1533e+01,  5.6518e+00],\n",
      "        [-5.3633e+00,  9.5052e+00,  8.5362e-01,  1.2828e+01, -1.0198e+01,\n",
      "         -2.2072e+00, -1.5676e+01, -2.7391e+00, -1.8419e-01,  5.6966e-01],\n",
      "        [-5.4520e-01,  6.3207e+00,  3.8277e+00,  1.3973e+01, -1.5152e+00,\n",
      "         -6.8689e+00, -1.3365e+01,  9.8421e+00,  1.4740e+01,  1.2942e+01],\n",
      "        [ 2.2689e+00,  1.1150e+01, -2.4279e-01,  1.6572e+01, -1.4735e+01,\n",
      "         -1.1412e+00, -2.0295e+01,  1.5139e+00,  9.9467e+00,  1.7261e+01],\n",
      "        [ 3.0609e+00,  5.3752e+00, -8.6619e+00,  1.2141e+01, -3.2329e+00,\n",
      "         -3.4438e+00, -1.6206e+01, -2.8097e+00,  1.1789e+01,  7.3719e+00],\n",
      "        [ 6.8270e+00,  1.3962e+01,  4.9136e+00,  1.3942e+01, -1.1461e+01,\n",
      "         -3.5601e+00, -1.1735e+01,  4.3909e+00, -1.3329e+00,  1.1354e+00],\n",
      "        [-1.8271e+00,  1.5915e+01,  5.7507e-01,  1.3434e+01, -1.6011e+01,\n",
      "          1.7460e+00, -1.7432e+01,  4.7779e-01,  8.6917e+00,  1.7422e+00],\n",
      "        [ 3.0596e-01,  4.9232e+00,  1.0353e+01,  9.6420e+00, -1.3524e+01,\n",
      "         -3.2902e+00, -1.6701e+01, -1.4056e+01,  1.3223e+01,  6.2856e+00],\n",
      "        [-1.1687e+00,  8.1795e+00, -7.2345e+00,  1.2604e+01, -1.5303e+01,\n",
      "         -2.0368e-01, -8.9404e+00, -6.4585e-01,  3.4268e+00, -1.2273e+00],\n",
      "        [ 6.0485e+00,  5.3784e+00,  5.1973e+00,  6.2174e-01, -8.9005e+00,\n",
      "         -6.4512e+00, -1.4387e+01,  1.4352e+00,  8.5693e+00,  1.1865e+01],\n",
      "        [-1.5121e+00,  1.4100e+01, -2.6796e+00,  1.1292e+01, -5.7271e+00,\n",
      "         -5.6281e+00, -9.2404e+00, -1.2552e+00,  4.6918e+00,  5.9914e+00],\n",
      "        [-4.3710e+00,  1.1921e+01,  2.5640e+00,  1.5142e+01, -8.0444e+00,\n",
      "         -1.6518e+00, -1.1106e+01,  8.8188e-01,  8.7795e-01,  9.4238e+00],\n",
      "        [-5.4893e+00,  7.3020e+00, -6.0077e-01,  7.7944e+00, -7.3081e+00,\n",
      "          5.6441e+00, -1.0525e+01,  5.9321e-01,  2.0875e+00,  1.4748e+00],\n",
      "        [-2.7407e+00,  1.1175e+01,  1.1906e+00,  5.5345e+00, -7.1447e+00,\n",
      "          1.4269e-02, -1.1201e+01, -5.2478e-01,  2.4413e+00, -4.6463e-01],\n",
      "        [-7.2694e+00,  2.0491e+01, -9.9162e+00,  1.2565e+01,  2.0384e+00,\n",
      "         -1.2033e+01, -1.3293e+01,  6.0146e+00,  1.3610e+01, -7.0099e+00],\n",
      "        [ 8.9040e+00,  5.4497e+00,  2.4110e+00,  9.1147e+00, -9.8004e+00,\n",
      "         -8.8865e+00, -1.3764e+01, -5.1630e+00,  8.6840e+00,  2.6515e+00],\n",
      "        [-6.7236e-01,  1.4924e+01, -1.1572e+00,  1.1562e+01, -1.5265e+01,\n",
      "         -4.4374e+00, -1.6309e+01,  2.6363e+00,  1.2643e+01,  7.4282e+00],\n",
      "        [ 8.8667e+00,  4.1538e+00,  1.9286e+00,  1.0439e+01, -4.4296e+00,\n",
      "         -4.3775e+00, -1.5762e+01,  1.0427e+01,  3.7846e-01,  8.7300e+00],\n",
      "        [ 3.1190e+00,  1.1164e+01, -1.0738e+00,  1.0045e+01, -3.2690e+00,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          3.5082e+00, -1.5746e+01, -4.7840e+00,  9.5252e+00, -7.5188e+00]])\n"
     ]
    }
   ],
   "source": [
    "## Your solution\n",
    "np.random.seed(7)\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + torch.exp(-x)) # or np.exp(-x)\n",
    "\n",
    "# Flatten the input image\n",
    "inputs = images.view(images.shape[0], -1)# or images.view(64, 784)\n",
    "\n",
    "# Set network layer sizes\n",
    "n_input = inputs.shape[1] # 784\n",
    "n_hidden = 256\n",
    "n_output = 10\n",
    "\n",
    "# Create parameters\n",
    "W1 = torch.randn(n_input, n_hidden) # weights for the hidden layer- 784X256\n",
    "W2 = torch.randn(n_hidden, n_output) # weights for the output layer- 256X10\n",
    "\n",
    "B1 = torch.randn(1, n_hidden) # biases for the hidden layer- 1X256\n",
    "B2 = torch.randn(1, n_output) # biases for the output layer- 1X10\n",
    "\n",
    "# output of hidden layer- 64X256 ie 1X256 values for 64 images in the batch\n",
    "# torch.mm(inputs, W1)- 64X256\n",
    "# even though B1 is of shape 1X256, it broadcasts to 64X256 when made to add with a 64X256 matrix\n",
    "h = activation(torch.mm(inputs, W1) + B1) \n",
    "\n",
    "# torch.mm(h, W2)- 64X10\n",
    "# B2 broadcasts from 1X10 to 64X10 on addition with a 64X10 matrix\n",
    "out = torch.mm(h, W2) + B2 # network output- 64X10 ie 10 output values for each of 64 images\n",
    "\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Softmax from SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    num = torch.exp(x)\n",
    "    den = torch.sum(torch.exp(x), dim=1).view(-1, 1) # change shape from (64, ) to (64, 1).\n",
    "    return num/den\n",
    "\n",
    "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/softmax-formula.png\" width=\"360px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is calculated by:\n",
    "\n",
    "1. Find the exponential of the quantity whose softmax score we need to know( `Numerator` )\n",
    "2. Find the exponential of all the quantities in the group over which we need to find the softmax score( the value of each of the 10 output nodes corresponding to 10 class labels in our case ) and add( $\\Sigma_j$ ) it ( `Denominator` )\n",
    "\n",
    "ie for each value in the group, calculate its exponential and divide it by the sum of exponentials of all values in its group\n",
    "\n",
    "In our case, we are calculating softmax for the output of all 64 images together. \n",
    "\n",
    "Let's start out by viewing how we calculate softmax for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([  1.2275,  17.8675,  -3.3610,   9.6572,  -4.4323,   2.3538, -17.0929,\n",
      "         11.1017,   7.5253,   6.0052])\n"
     ]
    }
   ],
   "source": [
    "# from our NN output of 64 images, get output of the first image\n",
    "# out.shape == [64, 10]\n",
    "single_image_output = out[0] # or out[0, :] as we require all the 10 column values in the first image\n",
    "\n",
    "print(single_image_output.shape)\n",
    "print(single_image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([3.4127e+00, 5.7512e+07, 3.4701e-02, 1.5635e+04, 1.1887e-02, 1.0525e+01,\n",
      "        3.7727e-08, 6.6287e+04, 1.8543e+03, 4.0552e+02])\n"
     ]
    }
   ],
   "source": [
    "# now we can find the softmax scores of the values in the output\n",
    "\n",
    "# 1. Numerator\n",
    "# instead of finding the exponential of each of the values in the output, we can find exponential of all values in the\n",
    "# output using vectorised operations\n",
    "numerator = torch.exp(single_image_output)\n",
    "\n",
    "print(numerator.shape)\n",
    "print(numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(57596524.)\n"
     ]
    }
   ],
   "source": [
    "# 2. Denominator\n",
    "# we need to find the sum of exponentials of all values in the output\n",
    "exponentials = torch.exp(single_image_output) # exponential of all values in the output as a tensor\n",
    "denominator = torch.sum(exponentials) # add all the exponential values to get the sum\n",
    "\n",
    "print(denominator.shape)\n",
    "print(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted denominator: tensor([57596524., 57596524., 57596524., 57596524., 57596524., 57596524.,\n",
      "        57596524., 57596524., 57596524., 57596524.]) of shape torch.Size([10])\n",
      "\n",
      "Softmax: tensor([5.9251e-08, 9.9854e-01, 6.0249e-10, 2.7145e-04, 2.0639e-10, 1.8274e-07,\n",
      "        6.5501e-16, 1.1509e-03, 3.2195e-05, 7.0407e-06])\n",
      "Softmax using denominator of same shape as numerator: tensor([5.9251e-08, 9.9854e-01, 6.0249e-10, 2.7145e-04, 2.0639e-10, 1.8274e-07,\n",
      "        6.5501e-16, 1.1509e-03, 3.2195e-05, 7.0407e-06])\n",
      "\n",
      "Both tensors are of same value\n"
     ]
    }
   ],
   "source": [
    "# Find softmax\n",
    "\n",
    "softmax = numerator / denominator\n",
    "\n",
    "# By dividing the numerator tensor of size 10( 1 dimension ) with denominator tensor of size 1 ( 0 dimension or scalar )\n",
    "# the denominator gets broadcasted to a suitable shape, here to size 10 to make the division happen\n",
    "\n",
    "# ie by running numerator / denominator actually denominator gets modified to\n",
    "broadcasted_denominator = torch.Tensor(10).fill_(denominator)\n",
    "\n",
    "print(f'Broadcasted denominator: {broadcasted_denominator} of shape {broadcasted_denominator.shape}\\n')\n",
    "\n",
    "softmax_broadcasted = numerator / broadcasted_denominator\n",
    "\n",
    "print(f'Softmax: {softmax}')\n",
    "print(f'Softmax using denominator of same shape as numerator: {softmax_broadcasted}\\n')\n",
    "\n",
    "\n",
    "equality_check = torch.all(torch.eq(softmax, softmax_broadcasted))\n",
    "if equality_check == 1:\n",
    "    print('Both tensors are of same value')\n",
    "else:\n",
    "    print('The tensors differ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The takeaway is that given 10 values to softmax, we use all these 10 values to calculate new 10 values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get on to calculating softmax over the batch of 64 images\n",
    "<img src=\"../img/softmax-formula.png\" width=\"360px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape) # we have 10 class labels each for the 64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# 1. Numerator\n",
    "# Find the exponential of all values we have\n",
    "numerator = torch.exp(out)\n",
    "print(num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 2. Denominator\n",
    "# find exponentials\n",
    "expos = torch.exp(out)\n",
    "# we need to add the 10 exponentials of each image and \n",
    "# get a new tensor with a sum for each image ie a 1D tensor of 64 values\n",
    "denominator = torch.sum(expos, dim=1) # dim=1 adds up all values in a row, ie for each image\n",
    "print(denominator.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expos is a `[64, 10]` tensor. `dim=1` in the `torch.sum` function tells it to sum over the values in the second dimension ( first dimenstion is specified by dim=0 ). This results in a `[64]` tensor ie a 1D tensor. \n",
    "\n",
    "If we had given `dim=0` as the argument, the returned tensor would had a shape of `[10]`, as the values in the first dimension would be added up.\n",
    "\n",
    "Incase you are wondering what would happen, if we omit the `dim` argument, it would just sum up all the values in the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.Size([64])** ~ **numpy (64, )** it is considered to be **(1, 64)** during broadcasting.\n",
    "\n",
    "So if we divide (64, 10) the numerator with (64, ), python will try to broadcast to make the tensors of same size.\n",
    "(64, ) to (1, 64) to ?\n",
    "\n",
    "(64, 10) and (1, 64) can't be broadcasted to equal size and python outputs error.\n",
    "\n",
    "Now if we reshape (64, ) to (64, 1), the broadcasting goes like this,\n",
    "(64, 1) to (64, 10)\n",
    "\n",
    "Now, (64, 10) the numerator and (64, 10) the broadcasted denominator can be divided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating ready-made NN( inheriting torch.nn.Module class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network3L(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your solution here\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# 3 layer NN\n",
    "class Network3L(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # first fc hidden layer\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "\n",
    "        # second fc hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # fc output layer\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x)) # output of first hidden layer\n",
    "        h2 = F.relu(self.fc2(h1)) # output of second hidden layer\n",
    "        out = F.sigmoid(self.fc3(h2)) # neural network output\n",
    "        \n",
    "model = Network3L()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight.shape, model.fc1.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Creating ready-made NN( using ***nn.Sequential*** )\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader)) # get first batch of images and their labels\n",
    "images.resize_(images.shape[0], 1, 784) # resize images in-place using resize_\n",
    "ps = model.forward(images[0,:]) # forward pass through our NN, the batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 9))\n",
    "\n",
    "ax[0].imshow(images[0].view(28, 28))\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].barh(np.arange(10), ps.data.numpy().squeeze())\n",
    "ax[1].set_aspect(0.1)\n",
    "ax[1].set_xlim(0, 1.1)\n",
    "ax[1].set_yticks(np.arange(10))\n",
    "ax[1].set_yticklabels(np.arange(10))\n",
    "ax[1].set_title('Class Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "#helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Losses in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the `nn` module, PyTorch provides losses such as the cross-entropy loss (`nn.CrossEntropyLoss`). With a classification problem such as MNIST digit classification, we're using the softmax function to predict class probabilities. With a softmax output, we use cross-entropy as the loss. To actually calculate the loss, you first define the criterion( the type of loss to use ) then pass in the output of your network and the correct labels.\n",
    "\n",
    "Something really important to note here. Looking at [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    ">\n",
    "> The input is expected to contain scores for each class.\n",
    "\n",
    "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the *logits* or *scores*. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more convenient to build the model with a log-softmax output using `nn.LogSoftmax` or `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Then you can get the actual probabilities by taking the exponential `torch.exp(output)`. With a log-softmax output, you want to use the negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "This can be used for example while validating the model, as it speeds up our code\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Training a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:15<00:00, 59.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.7905099846279697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:15<00:00, 62.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7698619087049956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:15<00:00, 58.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5029714950112137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:16<00:00, 57.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.41770175499702566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:18<00:00, 51.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.37675641253114\n"
     ]
    }
   ],
   "source": [
    "# show progress of for loop\n",
    "from tqdm import tqdm \n",
    "from torch import optim\n",
    "\n",
    "# define the model\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# set Negative log likelihood loss as loss function\n",
    "# LogSoftmax => NLLLoss\n",
    "criterion = nn.NLLLoss()\n",
    "# set Stochastic Gradient Descent as the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "# loop over the entire data 5 times\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in tqdm(trainloader):\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model(images) # 1. forward pass\n",
    "        loss = criterion(logps, labels) # 2. calculate loss\n",
    "        loss.backward() # 3. Find gradients by back-propogating\n",
    "        optimizer.step() # 4. Update weights using gradients\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else: # executed after for loop completes its execution normally - w/o encountering a break statement\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Validating our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 took 32.9s Training loss: 1.01 Testing loss: 0.58 Testing accuracy: 85.98726654052734%\n",
      "Epoch 2 took 41.7s Training loss: 0.63 Testing loss: 0.67 Testing accuracy: 84.35509490966797%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-dbb5200aa3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from time import time\n",
    "\n",
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    start = time()\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        ## Implement the validation pass and print out the validation accuracy\n",
    "        with torch.no_grad():\n",
    "            for images_val, labels_val in testloader:\n",
    "                images_val = images_val.view(images_val.shape[0], -1)\n",
    "                log_ps_val = model(images_val)            \n",
    "                test_loss += criterion(log_ps_val, labels_val)\n",
    "\n",
    "                ps = torch.exp(log_ps_val)\n",
    "                top_ps, top_classes = ps.topk(1, dim=1)\n",
    "                equals = top_classes == labels_val.view(*top_classes.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "        \n",
    "        end = time()\n",
    "        running_time = end-start\n",
    "        \n",
    "        print(f'Epoch {e+1} took {running_time:.1f}s',\n",
    "              f'Training loss: {train_losses[-1]:.2f}',\n",
    "              f'Testing loss: {test_losses[-1]:.2f}',\n",
    "              f'Testing accuracy: {(accuracy/len(testloader))*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Finding accuracy alternatively\n",
    "a = labels_val == top_classes.view(*labels_val.shape)\n",
    "mean1 = torch.mean(a.type(torch.FloatTensor))\n",
    "\n",
    "b = labels_val.numpy().reshape(-1, 1)\n",
    "c = top_classes.numpy()\n",
    "\n",
    "mean2 = np.mean(b==c)\n",
    "\n",
    "print(mean1.item()==mean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest training loss on epoch 16: 2.31\n",
      "Lowest validation loss on epoch 15: 2.31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_loss_min = min(train_losses)\n",
    "val_loss_min = min(test_losses)\n",
    "\n",
    "train_index = train_losses.index(train_loss_min)\n",
    "val_index = test_losses.index(val_loss_min)\n",
    "\n",
    "print(f'Lowest training loss on epoch {train_index}: {train_loss_min:.2f}')\n",
    "print(f'Lowest validation loss on epoch {val_index}: {val_loss_min:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_losses, color='red', marker='o', label='Training loss')\n",
    "ax.plot(test_losses, color='black', marker='+', label='Validation loss')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nn.Dropout(p=0.2)` to drop nodes with a probability of 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use `model.eval()`. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with `model.train()`. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.\n",
    "\n",
    "```python\n",
    "# turn off gradients\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # validation pass here\n",
    "    for images, labels in testloader:\n",
    "        ...\n",
    "\n",
    "# set model back to train mode\n",
    "model.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
