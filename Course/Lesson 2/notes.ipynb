{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to PyTorch - Udacity\n",
    "\n",
    "1. `Reshaping Tensors`\n",
    "2. `Numpy to Torch and back`\n",
    "3. `Getting Datasets`\n",
    "4. `2-Layer NN from SCRATCH`\n",
    "5. `Softmax from SCRATCH`\n",
    "    * Broadcasting in Numpy INTUITION\n",
    "    \n",
    "    \n",
    "6. `Creating ready-made NN( inheriting torch.nn.Module class )`\n",
    "7. `Creating ready-made NN( using nn.Sequential )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reshaping Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let **weights** be a tensor\n",
    "\n",
    "* `weights.reshape(a, b)` will return a new tensor with the same data(points to the same memory location) as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.\n",
    "* `weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**. Here is a great forum thread to [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n",
    "* `weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8947]) tensor([-0.3227])\n",
      "id(x): 2204467581432, id(y): 2204497406712\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1)\n",
    "y = torch.randn(1)\n",
    "\n",
    "print(x, y)\n",
    "print(f'id(x): {id(x)}, id(y): {id(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.5720]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x = x + y # Normal operation \n",
    "print(f'x = {x} id(x) = {id(x)}') # New location for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.2493]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x += y # inplace operation\n",
    "print(f'x = {x} id(x) = {id(x)}') # existing location used(in-place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([-0.0734]) id(x) = 2204497407864\n"
     ]
    }
   ],
   "source": [
    "x.add_(y) # inplace operation\n",
    "print(f'x = {x} id(x) = {id(x)}') # existing location used(in-place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inplace operations in pytorch are always postfixed with a _ , like .add_() or .scatter_(). Python operations like += or *= are also inplace operations.**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numpy to Torch and back\n",
    "\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62610611, 0.73824629, 0.68162487],\n",
       "       [0.45799142, 0.81512616, 0.9046208 ],\n",
       "       [0.13016165, 0.132936  , 0.03448641],\n",
       "       [0.46551398, 0.73371913, 0.05358193]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6261, 0.7382, 0.6816],\n",
       "        [0.4580, 0.8151, 0.9046],\n",
       "        [0.1302, 0.1329, 0.0345],\n",
       "        [0.4655, 0.7337, 0.0536]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62610611, 0.73824629, 0.68162487],\n",
       "       [0.45799142, 0.81512616, 0.9046208 ],\n",
       "       [0.13016165, 0.132936  , 0.03448641],\n",
       "       [0.46551398, 0.73371913, 0.05358193]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2522, 1.4765, 1.3632],\n",
       "        [0.9160, 1.6303, 1.8092],\n",
       "        [0.2603, 0.2659, 0.0690],\n",
       "        [0.9310, 1.4674, 0.1072]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.25221221, 1.47649258, 1.36324974],\n",
       "       [0.91598285, 1.63025232, 1.8092416 ],\n",
       "       [0.26032331, 0.26587199, 0.06897282],\n",
       "       [0.93102797, 1.46743827, 0.10716386]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = images.view(images.shape[0], -1)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2-Layer NN from SCRATCH\n",
    "\n",
    "Build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer and leave the output without one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([[ 1.2275e+00,  1.7868e+01, -3.3610e+00,  9.6572e+00, -4.4323e+00,\n",
      "          2.3538e+00, -1.7093e+01,  1.1102e+01,  7.5253e+00,  6.0052e+00],\n",
      "        [ 1.9197e+00,  3.9145e+00, -4.1184e+00,  1.3343e+01, -1.7035e+01,\n",
      "         -7.0955e+00, -2.2273e+01,  6.2673e+00, -9.8147e-01,  1.0414e+01],\n",
      "        [ 8.1331e+00,  1.6488e+01, -2.4356e-01,  7.5493e+00, -7.8329e+00,\n",
      "          7.8338e+00, -9.8890e+00, -3.9259e+00,  9.7671e+00, -1.7121e+00],\n",
      "        [-8.2881e+00,  8.3918e+00,  2.4722e+00,  1.5421e+01, -5.5535e+00,\n",
      "         -1.6099e+01, -1.6919e+01,  8.2451e+00,  7.3715e+00,  6.4537e+00],\n",
      "        [-3.0707e+00,  1.1511e+01,  5.7331e+00,  9.1911e+00, -1.1015e+01,\n",
      "         -1.7204e+01, -1.1893e+01,  1.2052e+01,  5.1869e+00,  5.8618e+00],\n",
      "        [ 8.3273e+00,  2.2861e+01,  5.9417e+00,  4.5867e+00, -8.6057e-01,\n",
      "          4.6767e-01, -5.5814e+00,  2.7008e+00,  5.5566e+00, -1.2963e+00],\n",
      "        [ 6.5105e+00,  1.0396e+01,  1.5333e+00,  9.1974e+00, -1.2533e+01,\n",
      "         -5.9950e+00, -9.0925e+00,  5.3603e+00,  9.2596e-01,  8.9151e+00],\n",
      "        [-1.3448e+00,  1.4160e+01, -3.9251e-01,  1.1068e+01, -1.1941e+01,\n",
      "          4.1885e+00, -1.2755e+01,  1.1375e+00,  4.4876e+00, -2.5309e+00],\n",
      "        [-2.0910e-02,  7.1224e+00, -1.8648e+00,  6.9049e+00, -7.3644e+00,\n",
      "         -8.4432e+00, -7.2797e+00, -1.0689e+01,  1.0118e+01,  8.2918e+00],\n",
      "        [-5.9524e+00,  1.5381e+01, -5.1827e+00,  1.1216e+01, -1.1935e+01,\n",
      "          3.8111e+00, -1.1889e+01, -5.8222e+00,  5.1292e+00, -1.1950e+00],\n",
      "        [ 3.0741e+00,  6.2451e+00, -1.4843e+01,  8.5791e+00, -1.2331e+01,\n",
      "         -6.2563e+00, -1.4550e+01,  7.6374e-01, -2.9410e+00,  9.4737e-01],\n",
      "        [-9.6433e-01,  1.1856e+01, -2.1460e+00,  7.4837e+00, -5.3113e+00,\n",
      "          6.4434e+00, -2.4769e+01, -6.4022e+00,  5.6151e+00, -3.4949e-01],\n",
      "        [ 4.5334e+00,  1.2757e+01,  3.2517e+00,  1.4420e+01, -1.7858e+01,\n",
      "         -1.4930e+00, -1.6957e+01, -1.7111e+00,  4.3772e+00,  5.1503e-01],\n",
      "        [ 5.5636e+00,  1.0455e+01, -2.3118e+00,  1.4103e+01, -1.9643e+01,\n",
      "          8.1362e-01, -1.2627e+01,  1.5108e+00,  2.1505e+00, -1.0258e+00],\n",
      "        [ 1.7004e+01,  1.0788e+01,  1.0859e+01,  1.2053e+01, -1.1350e+01,\n",
      "         -1.4089e+01, -1.6695e+01,  1.0260e+01,  5.7475e+00,  7.7414e+00],\n",
      "        [-4.0792e+00,  1.1948e+01, -2.8409e+00,  1.1287e+01, -4.9033e+00,\n",
      "         -9.2723e+00, -4.2669e+00,  8.0237e+00, -6.6854e-02,  2.9975e+00],\n",
      "        [-2.9292e+00,  8.5190e+00,  8.8474e-01,  8.4142e+00, -4.9318e+00,\n",
      "         -1.9509e+00, -1.9224e+01,  1.8102e+00,  6.8603e+00, -9.2254e-01],\n",
      "        [ 5.1781e+00,  1.9556e+01, -3.5521e+00,  1.7280e+01, -1.7311e+00,\n",
      "         -3.6414e+00, -1.5468e+01,  6.6664e+00,  9.0824e+00,  7.2131e+00],\n",
      "        [ 6.1628e+00,  1.4270e+01, -8.5276e+00,  7.3227e+00, -1.0694e+01,\n",
      "         -4.3128e+00, -1.0635e+01,  4.5849e+00,  2.8945e+00,  9.5826e+00],\n",
      "        [ 5.8789e+00, -4.7597e-01, -3.4200e+00,  7.4879e+00, -7.0667e+00,\n",
      "         -5.7030e+00, -1.3984e+01, -1.8884e+00,  9.1656e+00,  5.0976e+00],\n",
      "        [ 3.7618e+00,  2.3547e+01, -3.4180e-01,  1.0696e+01, -6.1304e+00,\n",
      "          4.1629e-01, -1.3966e+01,  2.8627e+00,  5.2352e+00,  3.8309e+00],\n",
      "        [-1.0740e+00,  1.4181e+01, -7.4652e+00,  9.4380e+00, -1.0779e+01,\n",
      "         -4.4312e+00, -1.3663e+01,  2.3071e+00,  4.0605e+00,  1.1501e+01],\n",
      "        [ 8.4703e+00,  1.3298e+01,  1.1895e+00,  9.9966e+00, -4.9283e+00,\n",
      "         -2.1353e+00, -1.5580e+01,  3.9758e+00,  7.8917e+00, -4.9013e+00],\n",
      "        [ 1.3220e+00,  5.9659e+00,  8.8072e+00,  1.1382e+01, -5.4908e+00,\n",
      "         -3.7617e+00, -1.2509e+01,  1.6302e+00,  2.9756e-01,  1.3014e+01],\n",
      "        [ 5.7933e+00,  9.9503e+00, -3.5959e+00,  8.4717e+00, -1.4028e+01,\n",
      "          3.4240e+00, -1.7716e+01, -4.2401e+00,  5.8114e+00,  7.8385e+00],\n",
      "        [-1.6460e+00,  7.6542e+00, -9.1470e+00,  8.7447e+00, -1.1755e+01,\n",
      "          6.6537e-01, -1.5601e+01,  3.3210e+00,  3.3652e+00,  3.1737e+00],\n",
      "        [ 6.5072e+00,  2.4765e+01, -2.1521e+00,  9.4189e+00, -1.2246e+01,\n",
      "         -2.6575e+00, -1.3568e+01, -8.0640e+00,  7.6426e+00,  1.1201e+00],\n",
      "        [-4.5313e+00,  4.8037e+00, -4.9487e+00,  1.7642e+01, -5.6682e+00,\n",
      "         -5.1015e+00, -1.5955e+01, -2.5951e-01,  5.5312e+00,  1.3628e+01],\n",
      "        [-7.0681e-01,  1.4221e+01,  2.0482e+00,  6.7972e+00, -1.4271e+01,\n",
      "         -3.4375e+00, -8.1188e+00,  2.8644e+00,  1.0563e+00,  9.2446e+00],\n",
      "        [-3.3958e-01,  9.9580e+00, -1.2774e+01,  1.4012e+01, -8.3605e+00,\n",
      "         -7.6463e+00, -1.3899e+01, -1.5214e+01,  7.9080e+00, -7.9479e+00],\n",
      "        [ 6.5130e+00,  6.7572e+00, -8.9659e+00,  1.7952e+01, -1.2763e+01,\n",
      "         -3.9656e+00, -1.5732e+01,  4.8910e+00,  6.4145e+00,  1.0895e+01],\n",
      "        [-4.5738e+00,  1.3204e+01, -3.2457e+00,  5.8317e+00, -7.2960e+00,\n",
      "         -5.3711e+00, -1.4109e+01,  1.1324e+01,  1.5403e+00, -1.2591e+00],\n",
      "        [-3.0860e-01,  1.3455e+01, -2.3819e+00,  8.4273e+00, -9.2483e+00,\n",
      "         -2.3050e+00, -2.8579e+01,  9.8449e+00, -1.9906e+00,  1.1598e+01],\n",
      "        [ 1.3743e+01,  1.5186e+01, -2.1097e+00,  1.1746e+01, -1.4385e+01,\n",
      "         -8.6699e+00, -2.1463e+01, -2.1164e+00, -3.3917e+00,  6.7177e+00],\n",
      "        [ 2.4546e+00,  1.4810e+01, -2.2543e+00,  9.7937e+00,  5.4866e+00,\n",
      "         -4.0623e-01, -1.4811e+01, -3.4243e+00, -4.9088e-01,  3.0173e+00],\n",
      "        [-3.9211e+00,  1.7737e+01, -4.5324e+00,  1.1481e+01, -1.1865e+01,\n",
      "         -3.4042e+00, -1.5277e+01,  1.8949e+00,  2.5331e+00,  4.7170e+00],\n",
      "        [-6.3210e+00,  9.1610e+00, -3.7144e+00,  1.6855e+01, -1.0198e+01,\n",
      "          2.9097e+00, -1.9646e+01, -5.2721e+00,  3.7554e+00,  1.1439e+01],\n",
      "        [-1.7213e+00,  1.4224e+01,  6.4338e+00,  1.7415e+01, -8.3898e+00,\n",
      "         -5.8338e+00, -1.2869e+01,  1.0893e+01, -6.9176e-01,  2.2146e+00],\n",
      "        [ 2.3754e+00,  1.1297e+01,  1.4160e+00,  1.0938e+01, -6.0994e+00,\n",
      "          4.8036e+00, -1.4597e+01,  1.7066e+01,  2.6633e+00,  9.7969e+00],\n",
      "        [-1.7198e-01,  1.2201e+01,  6.5993e+00,  3.7973e+00, -1.7752e+01,\n",
      "         -2.4998e+00, -1.3387e+01,  7.7498e+00,  9.6752e+00,  3.4090e+00],\n",
      "        [-1.7270e+00,  1.0049e+01,  4.3021e+00,  1.0012e+01, -1.6718e+01,\n",
      "         -1.6270e+00, -1.2434e+01,  5.0648e-01,  6.5847e+00,  8.7103e+00],\n",
      "        [-3.9118e+00,  6.5003e+00, -1.4821e-01,  1.3538e+01, -9.5462e+00,\n",
      "          5.7484e+00, -1.5109e+01,  1.2891e+00,  5.4181e+00,  5.5239e+00],\n",
      "        [ 3.4923e+00,  7.0976e+00,  7.2264e+00,  1.7651e+01, -1.6620e+01,\n",
      "         -7.9887e+00, -1.5537e+01, -3.7094e-01,  8.2754e-02,  1.0746e+01],\n",
      "        [-3.3089e+00,  1.2866e+01,  3.8452e-01,  5.8648e+00, -1.6589e+01,\n",
      "         -1.8134e+00, -1.4402e+01, -5.2727e+00,  3.7298e-02, -3.6011e+00],\n",
      "        [-3.0780e+00,  1.5458e+00, -7.6187e+00,  9.2200e+00, -7.7033e+00,\n",
      "          6.7173e+00, -1.1830e+01, -1.1704e+00, -8.1303e+00,  1.0303e+01],\n",
      "        [-1.8077e+00,  7.7820e+00, -9.7886e+00,  8.2668e+00, -6.5495e+00,\n",
      "         -7.4815e+00, -1.6830e+01,  2.5089e+00,  1.1533e+01,  5.6518e+00],\n",
      "        [-5.3633e+00,  9.5052e+00,  8.5362e-01,  1.2828e+01, -1.0198e+01,\n",
      "         -2.2072e+00, -1.5676e+01, -2.7391e+00, -1.8419e-01,  5.6966e-01],\n",
      "        [-5.4520e-01,  6.3207e+00,  3.8277e+00,  1.3973e+01, -1.5152e+00,\n",
      "         -6.8689e+00, -1.3365e+01,  9.8421e+00,  1.4740e+01,  1.2942e+01],\n",
      "        [ 2.2689e+00,  1.1150e+01, -2.4279e-01,  1.6572e+01, -1.4735e+01,\n",
      "         -1.1412e+00, -2.0295e+01,  1.5139e+00,  9.9467e+00,  1.7261e+01],\n",
      "        [ 3.0609e+00,  5.3752e+00, -8.6619e+00,  1.2141e+01, -3.2329e+00,\n",
      "         -3.4438e+00, -1.6206e+01, -2.8097e+00,  1.1789e+01,  7.3719e+00],\n",
      "        [ 6.8270e+00,  1.3962e+01,  4.9136e+00,  1.3942e+01, -1.1461e+01,\n",
      "         -3.5601e+00, -1.1735e+01,  4.3909e+00, -1.3329e+00,  1.1354e+00],\n",
      "        [-1.8271e+00,  1.5915e+01,  5.7507e-01,  1.3434e+01, -1.6011e+01,\n",
      "          1.7460e+00, -1.7432e+01,  4.7779e-01,  8.6917e+00,  1.7422e+00],\n",
      "        [ 3.0596e-01,  4.9232e+00,  1.0353e+01,  9.6420e+00, -1.3524e+01,\n",
      "         -3.2902e+00, -1.6701e+01, -1.4056e+01,  1.3223e+01,  6.2856e+00],\n",
      "        [-1.1687e+00,  8.1795e+00, -7.2345e+00,  1.2604e+01, -1.5303e+01,\n",
      "         -2.0368e-01, -8.9404e+00, -6.4585e-01,  3.4268e+00, -1.2273e+00],\n",
      "        [ 6.0485e+00,  5.3784e+00,  5.1973e+00,  6.2174e-01, -8.9005e+00,\n",
      "         -6.4512e+00, -1.4387e+01,  1.4352e+00,  8.5693e+00,  1.1865e+01],\n",
      "        [-1.5121e+00,  1.4100e+01, -2.6796e+00,  1.1292e+01, -5.7271e+00,\n",
      "         -5.6281e+00, -9.2404e+00, -1.2552e+00,  4.6918e+00,  5.9914e+00],\n",
      "        [-4.3710e+00,  1.1921e+01,  2.5640e+00,  1.5142e+01, -8.0444e+00,\n",
      "         -1.6518e+00, -1.1106e+01,  8.8188e-01,  8.7795e-01,  9.4238e+00],\n",
      "        [-5.4893e+00,  7.3020e+00, -6.0077e-01,  7.7944e+00, -7.3081e+00,\n",
      "          5.6441e+00, -1.0525e+01,  5.9321e-01,  2.0875e+00,  1.4748e+00],\n",
      "        [-2.7407e+00,  1.1175e+01,  1.1906e+00,  5.5345e+00, -7.1447e+00,\n",
      "          1.4269e-02, -1.1201e+01, -5.2478e-01,  2.4413e+00, -4.6463e-01],\n",
      "        [-7.2694e+00,  2.0491e+01, -9.9162e+00,  1.2565e+01,  2.0384e+00,\n",
      "         -1.2033e+01, -1.3293e+01,  6.0146e+00,  1.3610e+01, -7.0099e+00],\n",
      "        [ 8.9040e+00,  5.4497e+00,  2.4110e+00,  9.1147e+00, -9.8004e+00,\n",
      "         -8.8865e+00, -1.3764e+01, -5.1630e+00,  8.6840e+00,  2.6515e+00],\n",
      "        [-6.7236e-01,  1.4924e+01, -1.1572e+00,  1.1562e+01, -1.5265e+01,\n",
      "         -4.4374e+00, -1.6309e+01,  2.6363e+00,  1.2643e+01,  7.4282e+00],\n",
      "        [ 8.8667e+00,  4.1538e+00,  1.9286e+00,  1.0439e+01, -4.4296e+00,\n",
      "         -4.3775e+00, -1.5762e+01,  1.0427e+01,  3.7846e-01,  8.7300e+00],\n",
      "        [ 3.1190e+00,  1.1164e+01, -1.0738e+00,  1.0045e+01, -3.2690e+00,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          3.5082e+00, -1.5746e+01, -4.7840e+00,  9.5252e+00, -7.5188e+00]])\n"
     ]
    }
   ],
   "source": [
    "## Your solution\n",
    "np.random.seed(7)\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + torch.exp(-x)) # or np.exp(-x)\n",
    "\n",
    "# Flatten the input image\n",
    "inputs = images.view(images.shape[0], -1)# or images.view(64, 784)\n",
    "\n",
    "# Set network layer sizes\n",
    "n_input = inputs.shape[1] # 784\n",
    "n_hidden = 256\n",
    "n_output = 10\n",
    "\n",
    "# Create parameters\n",
    "W1 = torch.randn(n_input, n_hidden) # weights for the hidden layer- 784X256\n",
    "W2 = torch.randn(n_hidden, n_output) # weights for the output layer- 256X10\n",
    "\n",
    "B1 = torch.randn(1, n_hidden) # biases for the hidden layer- 1X256\n",
    "B2 = torch.randn(1, n_output) # biases for the output layer- 1X10\n",
    "\n",
    "# output of hidden layer- 64X256 ie 1X256 values for 64 images in the batch\n",
    "# torch.mm(inputs, W1)- 64X256\n",
    "# even though B1 is of shape 1X256, it broadcasts to 64X256 when made to add with a 64X256 matrix\n",
    "h = activation(torch.mm(inputs, W1) + B1) \n",
    "\n",
    "# torch.mm(h, W2)- 64X10\n",
    "# B2 broadcasts from 1X10 to 64X10 on addition with a 64X10 matrix\n",
    "out = torch.mm(h, W2) + B2 # network output- 64X10 ie 10 output values for each of 64 images\n",
    "\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Softmax from SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    num = torch.exp(x)\n",
    "    den = torch.sum(torch.exp(x), dim=1).view(-1, 1) # change shape from (64, ) to (64, 1).\n",
    "    return num/den\n",
    "\n",
    "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
    "probabilities = softmax(out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/softmax-formula.png\" width=\"360px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is calculated by:\n",
    "\n",
    "1. Find the exponential of the quantity whose softmax score we need to know( `Numerator` )\n",
    "2. Find the exponential of all the quantities in the group over which we need to find the softmax score( the value of each of the 10 output nodes corresponding to 10 class labels in our case ) and add( $\\Sigma_j$ ) it ( `Denominator` )\n",
    "\n",
    "ie for each value in the group, calculate its exponential and divide it by the sum of exponentials of all values in its group\n",
    "\n",
    "In our case, we are calculating softmax for the output of all 64 images together. \n",
    "\n",
    "Let's start out by viewing how we calculate softmax for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([  1.2275,  17.8675,  -3.3610,   9.6572,  -4.4323,   2.3538, -17.0929,\n",
      "         11.1017,   7.5253,   6.0052])\n"
     ]
    }
   ],
   "source": [
    "# from our NN output of 64 images, get output of the first image\n",
    "# out.shape == [64, 10]\n",
    "single_image_output = out[0] # or out[0, :] as we require all the 10 column values in the first image\n",
    "\n",
    "print(single_image_output.shape)\n",
    "print(single_image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "tensor([3.4127e+00, 5.7512e+07, 3.4701e-02, 1.5635e+04, 1.1887e-02, 1.0525e+01,\n",
      "        3.7727e-08, 6.6287e+04, 1.8543e+03, 4.0552e+02])\n"
     ]
    }
   ],
   "source": [
    "# now we can find the softmax scores of the values in the output\n",
    "\n",
    "# 1. Numerator\n",
    "# instead of finding the exponential of each of the values in the output, we can find exponential of all values in the\n",
    "# output using vectorised operations\n",
    "numerator = torch.exp(single_image_output)\n",
    "\n",
    "print(numerator.shape)\n",
    "print(numerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(57596524.)\n"
     ]
    }
   ],
   "source": [
    "# 2. Denominator\n",
    "# we need to find the sum of exponentials of all values in the output\n",
    "exponentials = torch.exp(single_image_output) # exponential of all values in the output as a tensor\n",
    "denominator = torch.sum(exponentials) # add all the exponential values to get the sum\n",
    "\n",
    "print(denominator.shape)\n",
    "print(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasted denominator: tensor([57596524., 57596524., 57596524., 57596524., 57596524., 57596524.,\n",
      "        57596524., 57596524., 57596524., 57596524.]) of shape torch.Size([10])\n",
      "\n",
      "Softmax: tensor([5.9251e-08, 9.9854e-01, 6.0249e-10, 2.7145e-04, 2.0639e-10, 1.8274e-07,\n",
      "        6.5501e-16, 1.1509e-03, 3.2195e-05, 7.0407e-06])\n",
      "Softmax using denominator of same shape as numerator: tensor([5.9251e-08, 9.9854e-01, 6.0249e-10, 2.7145e-04, 2.0639e-10, 1.8274e-07,\n",
      "        6.5501e-16, 1.1509e-03, 3.2195e-05, 7.0407e-06])\n",
      "\n",
      "Both tensors are of same value\n"
     ]
    }
   ],
   "source": [
    "# Find softmax\n",
    "\n",
    "softmax = numerator / denominator\n",
    "\n",
    "# By dividing the numerator tensor of size 10( 1 dimension ) with denominator tensor of size 1 ( 0 dimension or scalar )\n",
    "# the denominator gets broadcasted to a suitable shape, here to size 10 to make the division happen\n",
    "\n",
    "# ie by running numerator / denominator actually denominator gets modified to\n",
    "broadcasted_denominator = torch.Tensor(10).fill_(denominator)\n",
    "\n",
    "print(f'Broadcasted denominator: {broadcasted_denominator} of shape {broadcasted_denominator.shape}\\n')\n",
    "\n",
    "softmax_broadcasted = numerator / broadcasted_denominator\n",
    "\n",
    "print(f'Softmax: {softmax}')\n",
    "print(f'Softmax using denominator of same shape as numerator: {softmax_broadcasted}\\n')\n",
    "\n",
    "\n",
    "equality_check = torch.all(torch.eq(softmax, softmax_broadcasted))\n",
    "if equality_check == 1:\n",
    "    print('Both tensors are of same value')\n",
    "else:\n",
    "    print('The tensors differ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The takeaway is that given 10 values to softmax, we use all these 10 values to calculate new 10 values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get on to calculating softmax over the batch of 64 images\n",
    "<img src=\"../img/softmax-formula.png\" width=\"360px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape) # we have 10 class labels each for the 64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# 1. Numerator\n",
    "# Find the exponential of all values we have\n",
    "numerator = torch.exp(out)\n",
    "print(num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# 2. Denominator\n",
    "# find exponentials\n",
    "expos = torch.exp(out)\n",
    "# we need to add the 10 exponentials of each image and \n",
    "# get a new tensor with a sum for each image ie a 1D tensor of 64 values\n",
    "denominator = torch.sum(expos, dim=1) # dim=1 adds up all values in a row, ie for each image\n",
    "print(denominator.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expos is a `[64, 10]` tensor. `dim=1` in the `torch.sum` function tells it to sum over the values in the second dimension ( first dimenstion is specified by dim=0 ). This results in a `[64]` tensor ie a 1D tensor. \n",
    "\n",
    "If we had given `dim=0` as the argument, the returned tensor would had a shape of `[10]`, as the values in the first dimension would be added up.\n",
    "\n",
    "Incase you are wondering what would happen, if we omit the `dim` argument, it would just sum up all the values in the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.Size([64])** ~ **numpy (64, )** it is considered to be **(1, 64)** during broadcasting.\n",
    "\n",
    "So if we divide (64, 10) the numerator with (64, ), python will try to broadcast to make the tensors of same size.\n",
    "(64, ) to (1, 64) to ?\n",
    "\n",
    "(64, 10) and (1, 64) can't be broadcasted to equal size and python outputs error.\n",
    "\n",
    "Now if we reshape (64, ) to (64, 1), the broadcasting goes like this,\n",
    "(64, 1) to (64, 10)\n",
    "\n",
    "Now, (64, 10) the numerator and (64, 10) the broadcasted denominator can be divided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating ready-made NN( inheriting torch.nn.Module class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network3L(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your solution here\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# 3 layer NN\n",
    "class Network3L(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # first fc hidden layer\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "\n",
    "        # second fc hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # fc output layer\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x)) # output of first hidden layer\n",
    "        h2 = F.relu(self.fc2(h1)) # output of second hidden layer\n",
    "        out = F.sigmoid(self.fc3(h2)) # neural network output\n",
    "        \n",
    "model = Network3L()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight.shape, model.fc1.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Creating ready-made NN( using ***nn.Sequential*** )\n",
    "\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader)) # get first batch of images and their labels\n",
    "images.resize_(images.shape[0], 1, 784) # resize images in-place using resize_\n",
    "ps = model.forward(images[0,:]) # forward pass through our NN, the batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 9))\n",
    "\n",
    "ax[0].imshow(images[0].view(28, 28))\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].barh(np.arange(10), ps.data.numpy().squeeze())\n",
    "ax[1].set_aspect(0.1)\n",
    "ax[1].set_xlim(0, 1.1)\n",
    "ax[1].set_yticks(np.arange(10))\n",
    "ax[1].set_yticklabels(np.arange(10))\n",
    "ax[1].set_title('Class Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "#helper.view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
